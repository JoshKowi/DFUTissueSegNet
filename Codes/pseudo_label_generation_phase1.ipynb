{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6WOqEOop8wQ7"
   },
   "source": [
    "**Create predictions (pseudo labels) from unsupervised images (no labels) using the trained model obtained after the supervised phase.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wKYTa1YNahpR"
   },
   "outputs": [],
   "source": [
    "# get local path to project\n",
    "import json\n",
    "\n",
    "with open(\"config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "BASE_PATH = config[\"PROJECT_DIR\"]\n",
    "DATA_PATH = config[\"DATA_DIR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wWvUuJ7xZ17E"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset as BaseDataset\n",
    "import albumentations as albu\n",
    "import cv2\n",
    "import numpy as np\n",
    "import segmentation_models_pytorch as smp\n",
    "from segmentation_models_pytorch.utils import metrics, losses, base\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "import torch.nn.functional as F\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u1TTozpn-D8o"
   },
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nx_iN5gGarSj"
   },
   "outputs": [],
   "source": [
    "class Dataset(BaseDataset):\n",
    "    \"\"\"CamVid Dataset. Read images, apply augmentation and preprocessing transformations.\n",
    "\n",
    "    Args:\n",
    "        images_dir (str): path to images folder\n",
    "        masks_dir (str): path to segmentation masks folder\n",
    "        augmentation (albumentations.Compose): data transfromation pipeline\n",
    "            (e.g. flip, scale, etc.)\n",
    "        preprocessing (albumentations.Compose): data preprocessing\n",
    "            (e.g. noralization, shape manipulation, etc.)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            list_IDs,\n",
    "            images_dir,\n",
    "            preprocessing=None,\n",
    "            resize=(False, (256, 256)), # To resize, the first value has to be True\n",
    "            n_classes:int=4,\n",
    "    ):\n",
    "        self.ids = list_IDs\n",
    "        self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ids]\n",
    "\n",
    "        self.preprocessing = preprocessing\n",
    "        self.resize = resize\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "\n",
    "        # read data\n",
    "        image = cv2.imread(self.images_fps[i])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if self.resize[0]:\n",
    "            image = cv2.resize(image, self.resize[1], interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        # apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image)\n",
    "            image = sample['image']\n",
    "\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nz_YtNAI718k"
   },
   "source": [
    "## Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "99hNddHg702N"
   },
   "outputs": [],
   "source": [
    "def get_training_augmentation():\n",
    "    train_transform = [\n",
    "\n",
    "        albu.OneOf(\n",
    "            [\n",
    "                albu.HorizontalFlip(p=0.5),\n",
    "                albu.VerticalFlip(p=0.5),\n",
    "            ],\n",
    "            p=0.8,\n",
    "        ),\n",
    "\n",
    "        albu.OneOf(\n",
    "            [\n",
    "                albu.ShiftScaleRotate(scale_limit=0.5, rotate_limit=0, shift_limit=0, p=0.1, border_mode=0), # scale only\n",
    "                albu.ShiftScaleRotate(scale_limit=0, rotate_limit=30, shift_limit=0, p=0.1, border_mode=0), # rotate only\n",
    "                albu.ShiftScaleRotate(scale_limit=0, rotate_limit=0, shift_limit=0.1, p=0.6, border_mode=0), # shift only\n",
    "                albu.ShiftScaleRotate(scale_limit=0.5, rotate_limit=30, shift_limit=0.1, p=0.2, border_mode=0), # affine transform\n",
    "            ],\n",
    "            p=0.9,\n",
    "        ),\n",
    "\n",
    "        albu.OneOf(\n",
    "            [\n",
    "                albu.Perspective(p=0.2),\n",
    "                albu.GaussNoise(p=0.2),\n",
    "                albu.Sharpen(p=0.2),\n",
    "                albu.Blur(blur_limit=3, p=0.2),\n",
    "                albu.MotionBlur(blur_limit=3, p=0.2),\n",
    "            ],\n",
    "            p=0.5,\n",
    "        ),\n",
    "\n",
    "        albu.OneOf(\n",
    "            [\n",
    "                albu.CLAHE(p=0.25),\n",
    "                albu.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.25),\n",
    "                albu.RandomGamma(p=0.25),\n",
    "                albu.HueSaturationValue(p=0.25),\n",
    "            ],\n",
    "            p=0.3,\n",
    "        ),\n",
    "\n",
    "    ]\n",
    "\n",
    "    return albu.Compose(train_transform, p=0.9) # 90% augmentation probability\n",
    "\n",
    "\n",
    "def get_validation_augmentation():\n",
    "    \"\"\"Add paddings to make image shape divisible by 32\"\"\"\n",
    "    test_transform = [\n",
    "        # albu.PadIfNeeded(512, 512)\n",
    "    ]\n",
    "    return albu.Compose(test_transform)\n",
    "\n",
    "\n",
    "def to_tensor(x, **kwargs):\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "\n",
    "def get_preprocessing(preprocessing_fn):\n",
    "    \"\"\"Construct preprocessing transform\n",
    "\n",
    "    Args:\n",
    "        preprocessing_fn (callbale): data normalization function\n",
    "            (can be specific for each pretrained neural network)\n",
    "    Return:\n",
    "        transform: albumentations.Compose\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    _transform = [\n",
    "        albu.Lambda(image=preprocessing_fn),\n",
    "        albu.Lambda(image=to_tensor, mask=to_tensor),\n",
    "    ]\n",
    "    return albu.Compose(_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z9xxDHfp_yFO"
   },
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yq5o1vzBYxN0"
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "BASE_MODEL = 'MiT+pscse'\n",
    "ENCODER = 'mit_b3'\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "BATCH_SIZE = 16\n",
    "n_classes = 4\n",
    "ACTIVATION = 'sigmoid' # could be None for logits or 'softmax2d' for multiclass segmentation\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "LR = 0.0001 # learning rate\n",
    "EPOCHS = 500\n",
    "WEIGHT_DECAY = 1e-5\n",
    "SAVE_WEIGHTS_ONLY = True\n",
    "RESIZE = (False, (256,256)) # if resize needed\n",
    "TO_CATEGORICAL = True\n",
    "SAVE_BEST_MODEL = True\n",
    "SAVE_LAST_MODEL = False\n",
    "\n",
    "PERIOD = 10 # periodically save checkpoints\n",
    "RAW_PREDICTION = False # if true, then stores raw predictions (i.e. before applying threshold)\n",
    "RETRAIN = False\n",
    "\n",
    "# For early stopping\n",
    "EARLY_STOP = True # True to activate early stopping\n",
    "PATIENCE = 50 # for early stopping\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mp9ohG-nyZzg"
   },
   "source": [
    "\n",
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ja17lcFcb7wB"
   },
   "outputs": [],
   "source": [
    "# create segmentation model with pretrained encoder\n",
    "model = smp.Unet(\n",
    "    encoder_name=ENCODER,\n",
    "    encoder_weights=ENCODER_WEIGHTS,\n",
    "    # aux_params=aux_params,\n",
    "    classes=n_classes,\n",
    "    activation=ACTIVATION,\n",
    "    decoder_attention_type='pscse',\n",
    ")\n",
    "\n",
    "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n",
    "\n",
    "model.to(DEVICE)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam([\n",
    "    dict(params=model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY),\n",
    "])\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                              factor=0.1,\n",
    "                              mode='min',\n",
    "                              patience=10,\n",
    "                              min_lr=0.00001,\n",
    "                              verbose=True,\n",
    "                              )\n",
    "\n",
    "# Model name that will be loaded\n",
    "model_name = 'MiT+pscse_padded_aug_mit_b3_sup_2025-03-17_10-12-44'\n",
    "print(model_name)\n",
    "\n",
    "# Checkpoint directory\n",
    "checkpoint_loc = DATA_PATH + 'checkpoints/' + model_name\n",
    "\n",
    "# Load model====================================================================\n",
    "checkpoint = torch.load(os.path.join(checkpoint_loc, 'best_model.pth'))\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IGDNX6LI9zO8"
   },
   "source": [
    "## Read unsupervised image names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IALquySrcE7x"
   },
   "outputs": [],
   "source": [
    "x_test_dir = DATA_PATH + 'Unlabeled/'\n",
    "\n",
    "list_IDs_test = os.listdir(x_test_dir)\n",
    "\n",
    "print('No. of test images: ', len(list_IDs_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ivdoZtZP99lE"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MwRA6u_4ZvyE"
   },
   "outputs": [],
   "source": [
    "# Test dataloader ==============================================================\n",
    "test_dataset = Dataset(\n",
    "    list_IDs_test,\n",
    "    x_test_dir,\n",
    "    preprocessing=get_preprocessing(preprocessing_fn),\n",
    "    resize=(RESIZE),\n",
    "    n_classes=n_classes,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset,\n",
    "                            batch_size=1,\n",
    "                            shuffle=False,\n",
    "                            num_workers=6)\n",
    "\n",
    "# Prediction ===================================================================\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import scipy.io as sio\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "save_pred = True\n",
    "threshold = 0.5\n",
    "ep = 1e-6\n",
    "raw_pred = []\n",
    "\n",
    "HARD_LINE = True\n",
    "\n",
    "# Save directory\n",
    "save_dir_pred = DATA_PATH + 'predictions/' + model_name + '_selfsupervised'\n",
    "if not os.path.exists(save_dir_pred): os.makedirs(save_dir_pred)\n",
    "\n",
    "save_dir_pred_palette = DATA_PATH + 'predictions_palettte/' + model_name + '_selfsupervised'\n",
    "if not os.path.exists(save_dir_pred_palette): os.makedirs(save_dir_pred_palette)\n",
    "\n",
    "iter_test_dataloader = iter(test_dataloader)\n",
    "\n",
    "palette = [[128, 128, 128], [255, 0, 0], [0, 255, 0], [0, 0, 255]]\n",
    "\n",
    "for enu, i in enumerate(range(len(list_IDs_test))):\n",
    "\n",
    "    name = os.path.splitext(list_IDs_test[i])[0] # remove extension\n",
    "\n",
    "    print('Processing:', enu, name)\n",
    "\n",
    "    # Image-wise mean of metrics\n",
    "    i_mp, i_mr, i_mdice, i_miou = [], [], [], []\n",
    "\n",
    "    image = next(iter_test_dataloader) # get image and mask as Tensors\n",
    "\n",
    "    pr_mask = model.predict(image.to(DEVICE)) # Move image tensor to gpu\n",
    "\n",
    "    # Convert from onehot\n",
    "    # gt_mask = torch.argmax(gt_mask_, dim=1)\n",
    "    if TO_CATEGORICAL:\n",
    "        pr_mask = torch.argmax(pr_mask, dim=1)\n",
    "\n",
    "    # pr_mask = torch.argmax(pr_mask, dim=1)\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    pred = pr_mask.squeeze().cpu().numpy()\n",
    "\n",
    "    # Save raw prediction\n",
    "    if RAW_PREDICTION: raw_pred.append(pred)\n",
    "\n",
    "    # Save prediction as png\n",
    "    if save_pred:\n",
    "        \"Uncomment for non-palette\"\n",
    "        cv2.imwrite(os.path.join(save_dir_pred, list_IDs_test[i]), np.squeeze(pred).astype(np.uint8))\n",
    "\n",
    "        \"Uncomment for palette\"\n",
    "        # Palette prediction\n",
    "        pal_pred = np.squeeze(pred).astype(np.uint8)\n",
    "        pal_pred = Image.fromarray(pal_pred)\n",
    "        pal_pred = pal_pred.convert(\"P\")\n",
    "        pal_pred.putpalette(np.array(palette, dtype=np.uint8))\n",
    "\n",
    "        # Store\n",
    "        pal_pred.save(os.path.join(save_dir_pred_palette, list_IDs_test[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "cap dfu",
   "language": "python",
   "name": "cap_dfu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
